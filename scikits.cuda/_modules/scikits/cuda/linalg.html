<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>scikits.cuda.linalg &mdash; scikits.cuda 0.043 documentation</title>
    
    <link rel="stylesheet" href="../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/bootswatch-3.1.0/united/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/bootstrap-sphinx.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '0.043',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../../../_static/bootstrap-3.1.0/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../../../_static/bootstrap-sphinx.js"></script>
    <link rel="top" title="scikits.cuda 0.043 documentation" href="../../../index.html" />
    <link rel="up" title="Module code" href="../../index.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../index.html">
          scikits.cuda</a>
        <span class="navbar-text navbar-version pull-left"><b>0.043</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            <li class="divider-vertical"></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../index.html">Contents <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Installation Instructions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../install.html#quick-installation">Quick Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../install.html#obtaining-the-latest-software">Obtaining the Latest Software</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../install.html#installation-dependencies">Installation Dependencies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../install.html#platform-support">Platform Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../install.html#building-and-installation">Building and Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../install.html#running-the-unit-tests">Running the Unit Tests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../install.html#getting-started">Getting Started</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../reference.html">Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../reference.html#library-wrapper-routines">Library Wrapper Routines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../reference.html#high-level-routines">High-Level Routines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../reference.html#other-routines">Other Routines</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../authors.html">Authors &amp; Acknowledgments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../license.html">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../changes.html">Change Log</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../changes.html#release-0-043-under-development">Release 0.043 - (under development)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../changes.html#release-0-042-march-10-2013">Release 0.042 - (March 10, 2013)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../changes.html#release-0-041-may-22-2011">Release 0.041 - (May 22, 2011)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../changes.html#release-0-04-may-11-2011">Release 0.04 - (May 11, 2011)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../changes.html#release-0-03-november-22-2010">Release 0.03 - (November 22, 2010)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../changes.html#release-0-02-september-21-2010">Release 0.02 - (September 21, 2010)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../changes.html#release-0-01-september-17-2010">Release 0.01 - (September 17, 2010)</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"></ul>
</li>
              
            
            
              
                
              
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="col-md-12">
      
  <h1>Source code for scikits.cuda.linalg</h1><div class="highlight"><pre>
<span class="c">#!/usr/bin/env python</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">PyCUDA-based linear algebra functions.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>
<span class="kn">from</span> <span class="nn">string</span> <span class="kn">import</span> <span class="n">Template</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span>
<span class="kn">from</span> <span class="nn">pycuda.compiler</span> <span class="kn">import</span> <span class="n">SourceModule</span>
<span class="kn">import</span> <span class="nn">pycuda.gpuarray</span> <span class="kn">as</span> <span class="nn">gpuarray</span>
<span class="kn">import</span> <span class="nn">pycuda.driver</span> <span class="kn">as</span> <span class="nn">drv</span>
<span class="kn">import</span> <span class="nn">pycuda.elementwise</span> <span class="kn">as</span> <span class="nn">el</span>
<span class="kn">import</span> <span class="nn">pycuda.tools</span> <span class="kn">as</span> <span class="nn">tools</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">cuda</span>
<span class="kn">import</span> <span class="nn">cublas</span>
<span class="kn">import</span> <span class="nn">misc</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">cula</span>
    <span class="n">_has_cula</span> <span class="o">=</span> <span class="bp">True</span>
<span class="k">except</span> <span class="p">(</span><span class="ne">ImportError</span><span class="p">,</span> <span class="ne">OSError</span><span class="p">):</span>
    <span class="n">_has_cula</span> <span class="o">=</span> <span class="bp">False</span>

<span class="kn">from</span> <span class="nn">misc</span> <span class="kn">import</span> <span class="n">init</span>

<span class="c"># Get installation location of C headers:</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">install_headers</span>

<div class="viewcode-block" id="svd"><a class="viewcode-back" href="../../../generated/scikits.cuda.linalg.svd.html#scikits.cuda.linalg.svd">[docs]</a><span class="k">def</span> <span class="nf">svd</span><span class="p">(</span><span class="n">a_gpu</span><span class="p">,</span> <span class="n">jobu</span><span class="o">=</span><span class="s">&#39;A&#39;</span><span class="p">,</span> <span class="n">jobvt</span><span class="o">=</span><span class="s">&#39;A&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Singular Value Decomposition.</span>

<span class="sd">    Factors the matrix `a` into two unitary matrices, `u` and `vh`,</span>
<span class="sd">    and a 1-dimensional array of real, non-negative singular values,</span>
<span class="sd">    `s`, such that `a == dot(u.T, dot(diag(s), vh.T))`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Input matrix of shape `(m, n)` to decompose.</span>
<span class="sd">    jobu : {&#39;A&#39;, &#39;S&#39;, &#39;O&#39;, &#39;N&#39;}</span>
<span class="sd">        If &#39;A&#39;, return the full `u` matrix with shape `(m, m)`.</span>
<span class="sd">        If &#39;S&#39;, return the `u` matrix with shape `(m, k)`.</span>
<span class="sd">        If &#39;O&#39;, return the `u` matrix with shape `(m, k) without</span>
<span class="sd">        allocating a new matrix.</span>
<span class="sd">        If &#39;N&#39;, don&#39;t return `u`.</span>
<span class="sd">    jobvt : {&#39;A&#39;, &#39;S&#39;, &#39;O&#39;, &#39;N&#39;}</span>
<span class="sd">        If &#39;A&#39;, return the full `vh` matrix with shape `(n, n)`.</span>
<span class="sd">        If &#39;S&#39;, return the `vh` matrix with shape `(k, n)`.</span>
<span class="sd">        If &#39;O&#39;, return the `vh` matrix with shape `(k, n) without</span>
<span class="sd">        allocating a new matrix.</span>
<span class="sd">        If &#39;N&#39;, don&#39;t return `vh`.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    u : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Unitary matrix of shape `(m, m)` or `(m, k)` depending on</span>
<span class="sd">        value of `jobu`.</span>
<span class="sd">    s : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Array containing the singular values, sorted such that `s[i] &gt;= s[i+1]`.</span>
<span class="sd">        `s` is of length `min(m, n)`.</span>
<span class="sd">    vh : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Unitary matrix of shape `(n, n)` or `(k, n)`, depending</span>
<span class="sd">        on `jobvt`.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Double precision is only supported if the standard version of the</span>
<span class="sd">    CULA Dense toolkit is installed.</span>

<span class="sd">    This function destroys the contents of the input matrix regardless</span>
<span class="sd">    of the values of `jobu` and `jobvt`.</span>

<span class="sd">    Only one of `jobu` or `jobvt` may be set to `O`, and then only for</span>
<span class="sd">    a square matrix.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.gpuarray as gpuarray</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.autoinit</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import linalg</span>
<span class="sd">    &gt;&gt;&gt; linalg.init()</span>
<span class="sd">    &gt;&gt;&gt; a = np.random.randn(9, 6) + 1j*np.random.randn(9, 6)</span>
<span class="sd">    &gt;&gt;&gt; a = np.asarray(a, np.complex64)</span>
<span class="sd">    &gt;&gt;&gt; a_gpu = gpuarray.to_gpu(a)</span>
<span class="sd">    &gt;&gt;&gt; u_gpu, s_gpu, vh_gpu = linalg.svd(a_gpu, &#39;S&#39;, &#39;S&#39;)</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(a, np.dot(u_gpu.get(), np.dot(np.diag(s_gpu.get()), vh_gpu.get())), 1e-4)</span>
<span class="sd">    True</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">_has_cula</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">NotImplementError</span><span class="p">(</span><span class="s">&#39;CULA not installed&#39;</span><span class="p">)</span>

    <span class="c"># The free version of CULA only supports single precision floating</span>
    <span class="c"># point numbers:</span>
    <span class="n">data_type</span> <span class="o">=</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">type</span>
    <span class="n">real_type</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span>
    <span class="k">if</span> <span class="n">data_type</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">:</span>
        <span class="n">cula_func</span> <span class="o">=</span> <span class="n">cula</span><span class="o">.</span><span class="n">_libcula</span><span class="o">.</span><span class="n">culaDeviceCgesvd</span>
    <span class="k">elif</span> <span class="n">data_type</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
        <span class="n">cula_func</span> <span class="o">=</span> <span class="n">cula</span><span class="o">.</span><span class="n">_libcula</span><span class="o">.</span><span class="n">culaDeviceSgesvd</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">cula</span><span class="o">.</span><span class="n">_libcula_toolkit</span> <span class="o">==</span> <span class="s">&#39;standard&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">data_type</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">:</span>
                <span class="n">cula_func</span> <span class="o">=</span> <span class="n">cula</span><span class="o">.</span><span class="n">_libcula</span><span class="o">.</span><span class="n">culaDeviceZgesvd</span>
            <span class="k">elif</span> <span class="n">data_type</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
                <span class="n">cula_func</span> <span class="o">=</span> <span class="n">cula</span><span class="o">.</span><span class="n">_libcula</span><span class="o">.</span><span class="n">culaDeviceDgesvd</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;unsupported type&#39;</span><span class="p">)</span>
            <span class="n">real_type</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;double precision not supported&#39;</span><span class="p">)</span>

    <span class="c"># Since CUDA assumes that arrays are stored in column-major</span>
    <span class="c"># format, the input matrix is assumed to be transposed:</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">square</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">==</span> <span class="n">m</span><span class="p">)</span>

    <span class="c"># Since the input matrix is transposed, jobu and jobvt must also</span>
    <span class="c"># be switched because the computed matrices will be returned in</span>
    <span class="c"># reversed order:</span>
    <span class="n">jobvt</span><span class="p">,</span> <span class="n">jobu</span> <span class="o">=</span> <span class="n">jobu</span><span class="p">,</span> <span class="n">jobvt</span>

    <span class="c"># Set the leading dimension of the input matrix:</span>
    <span class="n">lda</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>

    <span class="c"># Allocate the array of singular values:</span>
    <span class="n">s_gpu</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">real_type</span><span class="p">)</span>

    <span class="c"># Set the leading dimension and allocate u:</span>
    <span class="n">jobu</span> <span class="o">=</span> <span class="n">upper</span><span class="p">(</span><span class="n">jobu</span><span class="p">)</span>
    <span class="n">jobvt</span> <span class="o">=</span> <span class="n">upper</span><span class="p">(</span><span class="n">jobvt</span><span class="p">)</span>
    <span class="n">ldu</span> <span class="o">=</span> <span class="n">m</span>
    <span class="k">if</span> <span class="n">jobu</span> <span class="o">==</span> <span class="s">&#39;A&#39;</span><span class="p">:</span>
        <span class="n">u_gpu</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">ldu</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="n">data_type</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">jobu</span> <span class="o">==</span> <span class="s">&#39;S&#39;</span><span class="p">:</span>
        <span class="n">u_gpu</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="nb">min</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">ldu</span><span class="p">),</span> <span class="n">data_type</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">jobu</span> <span class="o">==</span> <span class="s">&#39;O&#39;</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">square</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;in-place computation of singular vectors &#39;</span><span class="o">+</span>
                             <span class="s">&#39;of non-square matrix not allowed&#39;</span><span class="p">)</span>
        <span class="n">ldu</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">u_gpu</span> <span class="o">=</span> <span class="n">a_gpu</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ldu</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">u_gpu</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">empty</span><span class="p">((),</span> <span class="n">data_type</span><span class="p">)</span>

    <span class="c"># Set the leading dimension and allocate vh:</span>
    <span class="k">if</span> <span class="n">jobvt</span> <span class="o">==</span> <span class="s">&#39;A&#39;</span><span class="p">:</span>
        <span class="n">ldvt</span> <span class="o">=</span> <span class="n">n</span>
        <span class="n">vh_gpu</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">data_type</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">jobvt</span> <span class="o">==</span> <span class="s">&#39;S&#39;</span><span class="p">:</span>
        <span class="n">ldvt</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
        <span class="n">vh_gpu</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">ldvt</span><span class="p">),</span> <span class="n">data_type</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">jobvt</span> <span class="o">==</span> <span class="s">&#39;O&#39;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">jobu</span> <span class="o">==</span> <span class="s">&#39;O&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;jobu and jobvt cannot both be O&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">square</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;in-place computation of singular vectors &#39;</span><span class="o">+</span>
                             <span class="s">&#39;of non-square matrix not allowed&#39;</span><span class="p">)</span>
        <span class="n">ldvt</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">vh_gpu</span> <span class="o">=</span> <span class="n">a_gpu</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ldvt</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">vh_gpu</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">empty</span><span class="p">((),</span> <span class="n">data_type</span><span class="p">)</span>

    <span class="c"># Compute SVD and check error status:</span>

    <span class="n">status</span> <span class="o">=</span> <span class="n">cula_func</span><span class="p">(</span><span class="n">jobu</span><span class="p">,</span> <span class="n">jobvt</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">),</span>
                       <span class="n">lda</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">s_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">u_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">),</span>
                       <span class="n">ldu</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">vh_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">),</span> <span class="n">ldvt</span><span class="p">)</span>

    <span class="n">cula</span><span class="o">.</span><span class="n">culaCheckStatus</span><span class="p">(</span><span class="n">status</span><span class="p">)</span>

    <span class="c"># Free internal CULA memory:</span>
    <span class="n">cula</span><span class="o">.</span><span class="n">culaFreeBuffers</span><span class="p">()</span>

    <span class="c"># Since the input is assumed to be transposed, it is necessary to</span>
    <span class="c"># return the computed matrices in reverse order:</span>
    <span class="k">if</span> <span class="n">jobu</span> <span class="ow">in</span> <span class="p">[</span><span class="s">&#39;A&#39;</span><span class="p">,</span> <span class="s">&#39;S&#39;</span><span class="p">,</span> <span class="s">&#39;O&#39;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">jobvt</span> <span class="ow">in</span> <span class="p">[</span><span class="s">&#39;A&#39;</span><span class="p">,</span> <span class="s">&#39;S&#39;</span><span class="p">,</span> <span class="s">&#39;O&#39;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">vh_gpu</span><span class="p">,</span> <span class="n">s_gpu</span><span class="p">,</span> <span class="n">u_gpu</span>
    <span class="k">elif</span> <span class="n">jobu</span> <span class="o">==</span> <span class="s">&#39;N&#39;</span> <span class="ow">and</span> <span class="n">jobvt</span> <span class="o">!=</span> <span class="s">&#39;N&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">vh_gpu</span><span class="p">,</span> <span class="n">s_gpu</span>
    <span class="k">elif</span> <span class="n">jobu</span> <span class="o">!=</span> <span class="s">&#39;N&#39;</span> <span class="ow">and</span> <span class="n">jobvt</span> <span class="o">==</span> <span class="s">&#39;N&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">s_gpu</span><span class="p">,</span> <span class="n">u_gpu</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">s_gpu</span>

</div>
<div class="viewcode-block" id="cho_factor"><a class="viewcode-back" href="../../../generated/scikits.cuda.linalg.cho_factor.html#scikits.cuda.linalg.cho_factor">[docs]</a><span class="k">def</span> <span class="nf">cho_factor</span><span class="p">(</span><span class="n">a_gpu</span><span class="p">,</span> <span class="n">uplo</span><span class="o">=</span><span class="s">&#39;L&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Cholesky factorisation</span>

<span class="sd">    Performs an in-place cholesky factorisation on the matrix `a`</span>
<span class="sd">    such that `a = x*x.T` or `x.T*x`, if the lower=&#39;L&#39; or upper=&#39;U&#39;</span>
<span class="sd">    triangle of `a` is used, respectively.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Input matrix of shape `(m, m)` to decompose.</span>
<span class="sd">    uplo: use the upper=&#39;U&#39; or lower=&#39;L&#39; (default) triangle of &#39;a&#39;</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    a: pycuda.gpuarray.GPUArray</span>
<span class="sd">        Cholesky factorised matrix</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Double precision is only supported if the standard version of the</span>
<span class="sd">    CULA Dense toolkit is installed.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.gpuarray as gpuarray</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.autoinit</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import scipy.linalg</span>
<span class="sd">    &gt;&gt;&gt; import linalg</span>
<span class="sd">    &gt;&gt;&gt; linalg.init()</span>
<span class="sd">    &gt;&gt;&gt; a = np.array([[3.0,0.0],[0.0,7.0]])</span>
<span class="sd">    &gt;&gt;&gt; a = np.asarray(a, np.float64)</span>
<span class="sd">    &gt;&gt;&gt; a_gpu = gpuarray.to_gpu(a)</span>
<span class="sd">    &gt;&gt;&gt; cho_factor(a_gpu)</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(a_gpu.get(), scipy.linalg.cho_factor(a)[0])</span>
<span class="sd">    True</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">_has_cula</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">NotImplementError</span><span class="p">(</span><span class="s">&#39;CULA not installed&#39;</span><span class="p">)</span>

    <span class="n">data_type</span> <span class="o">=</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">type</span>
    <span class="n">real_type</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span>
    <span class="k">if</span> <span class="n">cula</span><span class="o">.</span><span class="n">_libcula_toolkit</span> <span class="o">==</span> <span class="s">&#39;standard&#39;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">data_type</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">:</span>
            <span class="n">cula_func</span> <span class="o">=</span> <span class="n">cula</span><span class="o">.</span><span class="n">_libcula</span><span class="o">.</span><span class="n">culaDeviceCpotrf</span>
        <span class="k">elif</span> <span class="n">data_type</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
            <span class="n">cula_func</span> <span class="o">=</span> <span class="n">cula</span><span class="o">.</span><span class="n">_libcula</span><span class="o">.</span><span class="n">culaDeviceSpotrf</span>
        <span class="k">if</span> <span class="n">data_type</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">:</span>
            <span class="n">cula_func</span> <span class="o">=</span> <span class="n">cula</span><span class="o">.</span><span class="n">_libcula</span><span class="o">.</span><span class="n">culaDeviceZpotrf</span>
        <span class="k">elif</span> <span class="n">data_type</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
            <span class="n">cula_func</span> <span class="o">=</span> <span class="n">cula</span><span class="o">.</span><span class="n">_libcula</span><span class="o">.</span><span class="n">culaDeviceDpotrf</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;unsupported type&#39;</span><span class="p">)</span>
        <span class="n">real_type</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;Cholesky factorisation not included in CULA Dense Free version&#39;</span><span class="p">)</span>

    <span class="c"># Since CUDA assumes that arrays are stored in column-major</span>
    <span class="c"># format, the input matrix is assumed to be transposed:</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">square</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">==</span> <span class="n">m</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">n</span><span class="o">!=</span><span class="n">m</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;Matrix must be symmetric positive-definite&#39;</span><span class="p">)</span>

    <span class="c"># Set the leading dimension of the input matrix:</span>
    <span class="n">lda</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>

    <span class="n">status</span> <span class="o">=</span> <span class="n">cula_func</span><span class="p">(</span><span class="n">uplo</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">),</span> <span class="n">lda</span><span class="p">)</span>

    <span class="n">cula</span><span class="o">.</span><span class="n">culaCheckStatus</span><span class="p">(</span><span class="n">status</span><span class="p">)</span>

    <span class="c"># Free internal CULA memory:</span>
    <span class="n">cula</span><span class="o">.</span><span class="n">culaFreeBuffers</span><span class="p">()</span>

    <span class="c"># In-place operation. No return matrix. Result is stored in the input matrix.</span>

</div>
<div class="viewcode-block" id="cho_solve"><a class="viewcode-back" href="../../../generated/scikits.cuda.linalg.cho_solve.html#scikits.cuda.linalg.cho_solve">[docs]</a><span class="k">def</span> <span class="nf">cho_solve</span><span class="p">(</span><span class="n">a_gpu</span><span class="p">,</span> <span class="n">b_gpu</span><span class="p">,</span> <span class="n">uplo</span><span class="o">=</span><span class="s">&#39;L&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Cholesky solver</span>

<span class="sd">    Solve a system of equations via cholesky factorization,</span>
<span class="sd">    i.e. `a*x = b`.</span>
<span class="sd">    Overwrites `b` to give `inv(a)*b`, and overwrites the chosen triangle</span>
<span class="sd">    of `a` with factorized triangle</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Input matrix of shape `(m, m)` to decompose.</span>
<span class="sd">    b : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Input matrix of shape `(m, 1)` to decompose.</span>
<span class="sd">    uplo: chr</span>
<span class="sd">        use the upper=&#39;U&#39; or lower=&#39;L&#39; (default) triangle of `a`.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    a: pycuda.gpuarray.GPUArray</span>
<span class="sd">        Cholesky factorised matrix</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Double precision is only supported if the standard version of the</span>
<span class="sd">    CULA Dense toolkit is installed.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.gpuarray as gpuarray</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.autoinit</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import scipy.linalg</span>
<span class="sd">    &gt;&gt;&gt; import linalg</span>
<span class="sd">    &gt;&gt;&gt; linalg.init()</span>
<span class="sd">    &gt;&gt;&gt; a = np.array([[3.0,0.0],[0.0,7.0]])</span>
<span class="sd">    &gt;&gt;&gt; a = np.asarray(a, np.float64)</span>
<span class="sd">    &gt;&gt;&gt; a_gpu = gpuarray.to_gpu(a)</span>
<span class="sd">    &gt;&gt;&gt; b = np.array([11.,19.])</span>
<span class="sd">    &gt;&gt;&gt; b = np.asarray(b, np.float64)</span>
<span class="sd">    &gt;&gt;&gt; b_gpu  = gpuarray.to_gpu(b)</span>
<span class="sd">    &gt;&gt;&gt; cho_solve(a_gpu,b_gpu)</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(b_gpu.get(), scipy.linalg.cho_solve(scipy.linalg.cho_factor(a), b))</span>
<span class="sd">    True</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">_has_cula</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">NotImplementError</span><span class="p">(</span><span class="s">&#39;CULA not installed&#39;</span><span class="p">)</span>

    <span class="n">data_type</span> <span class="o">=</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">type</span>
    <span class="n">real_type</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span>
    <span class="k">if</span> <span class="n">cula</span><span class="o">.</span><span class="n">_libcula_toolkit</span> <span class="o">==</span> <span class="s">&#39;standard&#39;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">data_type</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">:</span>
            <span class="n">cula_func</span> <span class="o">=</span> <span class="n">cula</span><span class="o">.</span><span class="n">_libcula</span><span class="o">.</span><span class="n">culaDeviceCposv</span>
        <span class="k">elif</span> <span class="n">data_type</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
            <span class="n">cula_func</span> <span class="o">=</span> <span class="n">cula</span><span class="o">.</span><span class="n">_libcula</span><span class="o">.</span><span class="n">culaDeviceSposv</span>
        <span class="k">if</span> <span class="n">data_type</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">:</span>
            <span class="n">cula_func</span> <span class="o">=</span> <span class="n">cula</span><span class="o">.</span><span class="n">_libcula</span><span class="o">.</span><span class="n">culaDeviceZposv</span>
        <span class="k">elif</span> <span class="n">data_type</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
            <span class="n">cula_func</span> <span class="o">=</span> <span class="n">cula</span><span class="o">.</span><span class="n">_libcula</span><span class="o">.</span><span class="n">culaDeviceDposv</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;unsupported type&#39;</span><span class="p">)</span>
        <span class="n">real_type</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;Cholesky factorisation not included in CULA Dense Free version&#39;</span><span class="p">)</span>

    <span class="c"># Since CUDA assumes that arrays are stored in column-major</span>
    <span class="c"># format, the input matrix is assumed to be transposed:</span>
    <span class="n">na</span><span class="p">,</span> <span class="n">ma</span> <span class="o">=</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">square</span> <span class="o">=</span> <span class="p">(</span><span class="n">na</span> <span class="o">==</span> <span class="n">ma</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="p">(</span><span class="n">na</span><span class="o">!=</span><span class="n">ma</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;Matrix must be symmetric positive-definite&#39;</span><span class="p">)</span>

    <span class="c"># Set the leading dimension of the input matrix:</span>
    <span class="n">lda</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ma</span><span class="p">)</span>
    <span class="n">ldb</span> <span class="o">=</span> <span class="n">lda</span>

    <span class="c"># Assuming we are only solving for a vector. Hence, nrhs = 1</span>
    <span class="n">status</span> <span class="o">=</span> <span class="n">cula_func</span><span class="p">(</span><span class="n">uplo</span><span class="p">,</span> <span class="n">na</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">),</span> <span class="n">lda</span><span class="p">,</span> 
                       <span class="nb">int</span><span class="p">(</span><span class="n">b_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">),</span> <span class="n">ldb</span><span class="p">)</span>

    <span class="n">cula</span><span class="o">.</span><span class="n">culaCheckStatus</span><span class="p">(</span><span class="n">status</span><span class="p">)</span>

    <span class="c"># Free internal CULA memory:</span>
    <span class="n">cula</span><span class="o">.</span><span class="n">culaFreeBuffers</span><span class="p">()</span>

    <span class="c"># In-place operation. No return matrix. Result is stored in the input matrix</span>
    <span class="c"># and in the input vector.</span>

</div>
<div class="viewcode-block" id="dot"><a class="viewcode-back" href="../../../generated/scikits.cuda.linalg.dot.html#scikits.cuda.linalg.dot">[docs]</a><span class="k">def</span> <span class="nf">dot</span><span class="p">(</span><span class="n">x_gpu</span><span class="p">,</span> <span class="n">y_gpu</span><span class="p">,</span> <span class="n">transa</span><span class="o">=</span><span class="s">&#39;N&#39;</span><span class="p">,</span> <span class="n">transb</span><span class="o">=</span><span class="s">&#39;N&#39;</span><span class="p">,</span> <span class="n">handle</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dot product of two arrays.</span>

<span class="sd">    For 1D arrays, this function computes the inner product. For 2D</span>
<span class="sd">    arrays of shapes `(m, k)` and `(k, n)`, it computes the matrix</span>
<span class="sd">    product; the result has shape `(m, n)`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Input array.</span>
<span class="sd">    y_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Input array.</span>
<span class="sd">    transa : char</span>
<span class="sd">        If &#39;T&#39;, compute the product of the transpose of `x_gpu`.</span>
<span class="sd">        If &#39;C&#39;, compute the product of the Hermitian of `x_gpu`.</span>
<span class="sd">    transb : char</span>
<span class="sd">        If &#39;T&#39;, compute the product of the transpose of `y_gpu`.</span>
<span class="sd">        If &#39;C&#39;, compute the product of the Hermitian of `y_gpu`.</span>
<span class="sd">    handle : int</span>
<span class="sd">        CUBLAS context. If no context is specified, the default handle from</span>
<span class="sd">        `scikits.cuda.misc._global_cublas_handle` is used.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    c_gpu : pycuda.gpuarray.GPUArray, float{32,64}, or complex{64,128}</span>
<span class="sd">        Inner product of `x_gpu` and `y_gpu`. When the inputs are 1D</span>
<span class="sd">        arrays, the result will be returned as a scalar.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The input matrices must all contain elements of the same data type.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.gpuarray as gpuarray</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.autoinit</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import linalg</span>
<span class="sd">    &gt;&gt;&gt; import misc</span>
<span class="sd">    &gt;&gt;&gt; linalg.init()</span>
<span class="sd">    &gt;&gt;&gt; a = np.asarray(np.random.rand(4, 2), np.float32)</span>
<span class="sd">    &gt;&gt;&gt; b = np.asarray(np.random.rand(2, 2), np.float32)</span>
<span class="sd">    &gt;&gt;&gt; a_gpu = gpuarray.to_gpu(a)</span>
<span class="sd">    &gt;&gt;&gt; b_gpu = gpuarray.to_gpu(b)</span>
<span class="sd">    &gt;&gt;&gt; c_gpu = linalg.dot(a_gpu, b_gpu)</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(np.dot(a, b), c_gpu.get())</span>
<span class="sd">    True</span>
<span class="sd">    &gt;&gt;&gt; d = np.asarray(np.random.rand(5), np.float32)</span>
<span class="sd">    &gt;&gt;&gt; e = np.asarray(np.random.rand(5), np.float32)</span>
<span class="sd">    &gt;&gt;&gt; d_gpu = gpuarray.to_gpu(d)</span>
<span class="sd">    &gt;&gt;&gt; e_gpu = gpuarray.to_gpu(e)</span>
<span class="sd">    &gt;&gt;&gt; f = linalg.dot(d_gpu, e_gpu)</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(np.dot(d, e), f)</span>
<span class="sd">    True</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">handle</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">_global_cublas_handle</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>

        <span class="k">if</span> <span class="n">x_gpu</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="n">y_gpu</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;arrays must be of same length&#39;</span><span class="p">)</span>

        <span class="c"># Compute inner product for 1D arrays:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span> <span class="ow">and</span> <span class="n">y_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">):</span>
            <span class="n">cublas_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasCdotu</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span> <span class="ow">and</span> <span class="n">y_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
            <span class="n">cublas_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasSdot</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span> <span class="ow">and</span> <span class="n">y_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">):</span>
            <span class="n">cublas_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasZdotu</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="ow">and</span> <span class="n">y_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
            <span class="n">cublas_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasDdot</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;unsupported combination of input types&#39;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">cublas_func</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">x_gpu</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">x_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                           <span class="n">y_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>

        <span class="c"># Get the shapes of the arguments (accounting for the</span>
        <span class="c"># possibility that one of them may only have one dimension):</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">x_gpu</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">y_shape</span> <span class="o">=</span> <span class="n">y_gpu</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">x_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">y_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="c"># Perform matrix multiplication for 2D arrays:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span> <span class="ow">and</span> <span class="n">y_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">):</span>
            <span class="n">cublas_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasCgemm</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span> <span class="ow">and</span> <span class="n">y_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
            <span class="n">cublas_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasSgemm</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span> <span class="ow">and</span> <span class="n">y_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">):</span>
            <span class="n">cublas_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasZgemm</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="ow">and</span> <span class="n">y_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
            <span class="n">cublas_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasDgemm</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;unsupported combination of input types&#39;</span><span class="p">)</span>

        <span class="n">transa</span> <span class="o">=</span> <span class="n">lower</span><span class="p">(</span><span class="n">transa</span><span class="p">)</span>
        <span class="n">transb</span> <span class="o">=</span> <span class="n">lower</span><span class="p">(</span><span class="n">transb</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">transb</span> <span class="ow">in</span> <span class="p">[</span><span class="s">&#39;t&#39;</span><span class="p">,</span> <span class="s">&#39;c&#39;</span><span class="p">]:</span>
            <span class="n">m</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">y_shape</span>
        <span class="k">elif</span> <span class="n">transb</span> <span class="ow">in</span> <span class="p">[</span><span class="s">&#39;n&#39;</span><span class="p">]:</span>
            <span class="n">k</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">y_shape</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;invalid value for transb&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">transa</span> <span class="ow">in</span> <span class="p">[</span><span class="s">&#39;t&#39;</span><span class="p">,</span> <span class="s">&#39;c&#39;</span><span class="p">]:</span>
            <span class="n">l</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">x_shape</span>
        <span class="k">elif</span> <span class="n">transa</span> <span class="ow">in</span> <span class="p">[</span><span class="s">&#39;n&#39;</span><span class="p">]:</span>
            <span class="n">n</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">x_shape</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;invalid value for transa&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="n">k</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;objects are not aligned&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">transb</span> <span class="o">==</span> <span class="s">&#39;n&#39;</span><span class="p">:</span>
            <span class="n">lda</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lda</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">transa</span> <span class="o">==</span> <span class="s">&#39;n&#39;</span><span class="p">:</span>
            <span class="n">ldb</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ldb</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

        <span class="n">ldc</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>

        <span class="c"># Note that the desired shape of the output matrix is the transpose</span>
        <span class="c"># of what CUBLAS assumes:</span>
        <span class="n">c_gpu</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">ldc</span><span class="p">),</span> <span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">cublas_func</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">transb</span><span class="p">,</span> <span class="n">transa</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">y_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">,</span>
                    <span class="n">lda</span><span class="p">,</span> <span class="n">x_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">,</span> <span class="n">ldb</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">c_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">,</span> <span class="n">ldc</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">c_gpu</span>
</div>
<div class="viewcode-block" id="mdot"><a class="viewcode-back" href="../../../generated/scikits.cuda.linalg.mdot.html#scikits.cuda.linalg.mdot">[docs]</a><span class="k">def</span> <span class="nf">mdot</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Product of several matrices.</span>

<span class="sd">    Computes the matrix product of several arrays of shapes.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a_gpu, b_gpu, ... : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Arrays to multiply.</span>
<span class="sd">    handle : int</span>
<span class="sd">        CUBLAS context. If no context is specified, the default handle from</span>
<span class="sd">        `scikits.cuda.misc._global_cublas_handle` is used.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    c_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Matrix product of `a_gpu`, `b_gpu`, etc.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The input matrices must all contain elements of the same data type.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.gpuarray as gpuarray</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.autoinit</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import linalg</span>
<span class="sd">    &gt;&gt;&gt; linalg.init()</span>
<span class="sd">    &gt;&gt;&gt; a = np.asarray(np.random.rand(4, 2), np.float32)</span>
<span class="sd">    &gt;&gt;&gt; b = np.asarray(np.random.rand(2, 2), np.float32)</span>
<span class="sd">    &gt;&gt;&gt; c = np.asarray(np.random.rand(2, 2), np.float32)</span>
<span class="sd">    &gt;&gt;&gt; a_gpu = gpuarray.to_gpu(a)</span>
<span class="sd">    &gt;&gt;&gt; b_gpu = gpuarray.to_gpu(b)</span>
<span class="sd">    &gt;&gt;&gt; c_gpu = gpuarray.to_gpu(c)</span>
<span class="sd">    &gt;&gt;&gt; d_gpu = linalg.mdot(a_gpu, b_gpu, c_gpu)</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(np.dot(a, np.dot(b, c)), d_gpu.get())</span>
<span class="sd">    True</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">has_key</span><span class="p">(</span><span class="s">&#39;handle&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">kwargs</span><span class="p">[</span><span class="s">&#39;handle&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s">&#39;handle&#39;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">_global_cublas_handle</span>
        
    <span class="c"># Free the temporary matrix allocated when computing the dot</span>
    <span class="c"># product:</span>
    <span class="n">out_gpu</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">next_gpu</span> <span class="ow">in</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
        <span class="n">temp_gpu</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">out_gpu</span><span class="p">,</span> <span class="n">next_gpu</span><span class="p">,</span> <span class="n">handle</span><span class="o">=</span><span class="n">handle</span><span class="p">)</span>
        <span class="n">out_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="o">.</span><span class="n">free</span><span class="p">()</span>
        <span class="k">del</span><span class="p">(</span><span class="n">out_gpu</span><span class="p">)</span>
        <span class="n">out_gpu</span> <span class="o">=</span> <span class="n">temp_gpu</span>
        <span class="k">del</span><span class="p">(</span><span class="n">temp_gpu</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out_gpu</span>
</div>
<span class="k">def</span> <span class="nf">dot_diag</span><span class="p">(</span><span class="n">d_gpu</span><span class="p">,</span> <span class="n">a_gpu</span><span class="p">,</span> <span class="n">trans</span><span class="o">=</span><span class="s">&#39;N&#39;</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">handle</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dot product of diagonal and non-diagonal arrays.</span>

<span class="sd">    Computes the matrix product of a diagonal array represented as a</span>
<span class="sd">    vector and a non-diagonal array.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    d_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Array of length `N` corresponding to the diagonal of the</span>
<span class="sd">        multiplier.</span>
<span class="sd">    a_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Multiplicand array with shape `(N, M)`.</span>
<span class="sd">    trans : char</span>
<span class="sd">        If &#39;T&#39;, compute the product of the transpose of `a_gpu`.</span>
<span class="sd">    overwrite : bool</span>
<span class="sd">        If true (default), save the result in `a_gpu`.</span>
<span class="sd">    handle : int</span>
<span class="sd">        CUBLAS context. If no context is specified, the default handle from</span>
<span class="sd">        `scikits.cuda.misc._global_cublas_handle` is used.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    r_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        The computed matrix product.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    `d_gpu` and `a_gpu` must have the same precision data</span>
<span class="sd">    type. `d_gpu` may be real and `a_gpu` may be complex, but not</span>
<span class="sd">    vice-versa.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.autoinit</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.gpuarray as gpuarray</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import linalg</span>
<span class="sd">    &gt;&gt;&gt; linalg.init()</span>
<span class="sd">    &gt;&gt;&gt; d = np.random.rand(4)</span>
<span class="sd">    &gt;&gt;&gt; a = np.random.rand(4, 4)</span>
<span class="sd">    &gt;&gt;&gt; d_gpu = gpuarray.to_gpu(d)</span>
<span class="sd">    &gt;&gt;&gt; a_gpu = gpuarray.to_gpu(a)</span>
<span class="sd">    &gt;&gt;&gt; r_gpu = linalg.dot_diag(d_gpu, a_gpu)</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(np.dot(np.diag(d), a), r_gpu.get())</span>
<span class="sd">    True</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">handle</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">_global_cublas_handle</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">d_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;d_gpu must be a vector&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;a_gpu must be a matrix&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">lower</span><span class="p">(</span><span class="n">trans</span><span class="p">)</span> <span class="o">==</span> <span class="s">&#39;n&#39;</span><span class="p">:</span>
        <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span> <span class="o">=</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cols</span><span class="p">,</span> <span class="n">rows</span> <span class="o">=</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">d_gpu</span><span class="o">.</span><span class="n">size</span>
    <span class="k">if</span> <span class="n">N</span> <span class="o">!=</span> <span class="n">rows</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;incompatible dimensions&#39;</span><span class="p">)</span>

    <span class="n">float_type</span> <span class="o">=</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">type</span>
    <span class="k">if</span> <span class="n">float_type</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">d_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;precision of argument types must be the same&#39;</span><span class="p">)</span>
        <span class="n">scal_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasSscal</span>
        <span class="n">copy_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasScopy</span>
    <span class="k">elif</span> <span class="n">float_type</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">d_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;precision of argument types must be the same&#39;</span><span class="p">)</span>
        <span class="n">scal_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasDscal</span>
        <span class="n">copy_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasDcopy</span>
    <span class="k">elif</span> <span class="n">float_type</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">d_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">:</span>
            <span class="n">scal_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasCscal</span>
        <span class="k">elif</span> <span class="n">d_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
            <span class="n">scal_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasCsscal</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;precision of argument types must be the same&#39;</span><span class="p">)</span>
        <span class="n">copy_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasCcopy</span>
    <span class="k">elif</span> <span class="n">float_type</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">d_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">:</span>
            <span class="n">scal_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasZscal</span>
        <span class="k">elif</span> <span class="n">d_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
            <span class="n">scal_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasZdscal</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;precision of argument types must be the same&#39;</span><span class="p">)</span>
        <span class="n">copy_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasZcopy</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;unrecognized type&#39;</span><span class="p">)</span>

    <span class="n">d</span> <span class="o">=</span> <span class="n">d_gpu</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">overwrite</span><span class="p">:</span>
        <span class="n">r_gpu</span> <span class="o">=</span> <span class="n">a_gpu</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">r_gpu</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">a_gpu</span><span class="p">)</span>
        <span class="n">copy_func</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span>
                  <span class="nb">int</span><span class="p">(</span><span class="n">r_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">lower</span><span class="p">(</span><span class="n">trans</span><span class="p">)</span> <span class="o">==</span> <span class="s">&#39;n&#39;</span><span class="p">:</span>
        <span class="n">incx</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">bytes_step</span> <span class="o">=</span> <span class="n">cols</span><span class="o">*</span><span class="n">float_type</span><span class="p">()</span><span class="o">.</span><span class="n">itemsize</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">incx</span> <span class="o">=</span> <span class="n">rows</span>
        <span class="n">bytes_step</span> <span class="o">=</span> <span class="n">float_type</span><span class="p">()</span><span class="o">.</span><span class="n">itemsize</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">scal_func</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">d</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">int</span><span class="p">(</span><span class="n">r_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">)</span><span class="o">+</span><span class="n">i</span><span class="o">*</span><span class="n">bytes_step</span><span class="p">,</span> <span class="n">incx</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">r_gpu</span>

<span class="n">transpose_template</span> <span class="o">=</span> <span class="n">Template</span><span class="p">(</span><span class="s">&quot;&quot;&quot;</span>
<span class="s">#include &lt;pycuda-complex.hpp&gt;</span>

<span class="s">#if ${use_double}</span>
<span class="s">#if ${use_complex}</span>
<span class="s">#define FLOAT pycuda::complex&lt;double&gt;</span>
<span class="s">#define CONJ(x) conj(x)</span>
<span class="s">#else</span>
<span class="s">#define FLOAT double</span>
<span class="s">#define CONJ(x) (x)</span>
<span class="s">#endif</span>
<span class="s">#else</span>
<span class="s">#if ${use_complex}</span>
<span class="s">#define FLOAT pycuda::complex&lt;float&gt;</span>
<span class="s">#define CONJ(x) conj(x)</span>
<span class="s">#else</span>
<span class="s">#define FLOAT float</span>
<span class="s">#define CONJ(x) (x)</span>
<span class="s">#endif</span>
<span class="s">#endif</span>

<span class="s">__global__ void transpose(FLOAT *odata, FLOAT *idata, unsigned int N)</span>
<span class="s">{</span>
<span class="s">    unsigned int idx = blockIdx.y*blockDim.x*gridDim.x+</span>
<span class="s">                       blockIdx.x*blockDim.x+threadIdx.x;</span>
<span class="s">    unsigned int ix = idx/${cols};</span>
<span class="s">    unsigned int iy = idx%${cols};</span>

<span class="s">    if (idx &lt; N)</span>
<span class="s">        if (${hermitian})</span>
<span class="s">            odata[iy*${rows}+ix] = CONJ(idata[ix*${cols}+iy]);</span>
<span class="s">        else</span>
<span class="s">            odata[iy*${rows}+ix] = idata[ix*${cols}+iy];</span>
<span class="s">}</span>
<span class="s">&quot;&quot;&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="transpose"><a class="viewcode-back" href="../../../generated/scikits.cuda.linalg.transpose.html#scikits.cuda.linalg.transpose">[docs]</a><span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="n">a_gpu</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Matrix transpose.</span>

<span class="sd">    Transpose a matrix in device memory and return an object</span>
<span class="sd">    representing the transposed matrix.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Input matrix of shape `(m, n)`.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    at_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Transposed matrix of shape `(n, m)`.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The current implementation of the transpose operation is relatively inefficient.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.autoinit</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.driver as drv</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.gpuarray as gpuarray</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import linalg</span>
<span class="sd">    &gt;&gt;&gt; linalg.init()</span>
<span class="sd">    &gt;&gt;&gt; a = np.array([[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12]], np.float32)</span>
<span class="sd">    &gt;&gt;&gt; a_gpu = gpuarray.to_gpu(a)</span>
<span class="sd">    &gt;&gt;&gt; at_gpu = linalg.transpose(a_gpu)</span>
<span class="sd">    &gt;&gt;&gt; np.all(a.T == at_gpu.get())</span>
<span class="sd">    True</span>
<span class="sd">    &gt;&gt;&gt; b = np.array([[1j, 2j, 3j, 4j, 5j, 6j], [7j, 8j, 9j, 10j, 11j, 12j]], np.complex64)</span>
<span class="sd">    &gt;&gt;&gt; b_gpu = gpuarray.to_gpu(b)</span>
<span class="sd">    &gt;&gt;&gt; bt_gpu = linalg.transpose(b_gpu)</span>
<span class="sd">    &gt;&gt;&gt; np.all(b.T == bt_gpu.get())</span>
<span class="sd">    True</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;unrecognized type&#39;</span><span class="p">)</span>

    <span class="n">use_double</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">])</span>
    <span class="n">use_complex</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">])</span>

    <span class="c"># Get block/grid sizes:</span>
    <span class="n">dev</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">get_current_device</span><span class="p">()</span>
    <span class="n">block_dim</span><span class="p">,</span> <span class="n">grid_dim</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">select_block_grid_sizes</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c"># Set this to False when debugging to make sure the compiled kernel is</span>
    <span class="c"># not cached:</span>
    <span class="n">cache_dir</span><span class="o">=</span><span class="bp">None</span>
    <span class="n">transpose_mod</span> <span class="o">=</span> \
                  <span class="n">SourceModule</span><span class="p">(</span><span class="n">transpose_template</span><span class="o">.</span><span class="n">substitute</span><span class="p">(</span><span class="n">use_double</span><span class="o">=</span><span class="n">use_double</span><span class="p">,</span>
                                                             <span class="n">use_complex</span><span class="o">=</span><span class="n">use_complex</span><span class="p">,</span>
                                                             <span class="n">hermitian</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                               <span class="n">cols</span><span class="o">=</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                               <span class="n">rows</span><span class="o">=</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                               <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">)</span>

    <span class="n">transpose</span> <span class="o">=</span> <span class="n">transpose_mod</span><span class="o">.</span><span class="n">get_function</span><span class="p">(</span><span class="s">&quot;transpose&quot;</span><span class="p">)</span>
    <span class="n">at_gpu</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">transpose</span><span class="p">(</span><span class="n">at_gpu</span><span class="p">,</span> <span class="n">a_gpu</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">(</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
              <span class="n">block</span><span class="o">=</span><span class="n">block_dim</span><span class="p">,</span>
              <span class="n">grid</span><span class="o">=</span><span class="n">grid_dim</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">at_gpu</span>
</div>
<div class="viewcode-block" id="hermitian"><a class="viewcode-back" href="../../../generated/scikits.cuda.linalg.hermitian.html#scikits.cuda.linalg.hermitian">[docs]</a><span class="k">def</span> <span class="nf">hermitian</span><span class="p">(</span><span class="n">a_gpu</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hermitian (conjugate) matrix transpose.</span>

<span class="sd">    Conjugate transpose a matrix in device memory and return an object</span>
<span class="sd">    representing the transposed matrix.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Input matrix of shape `(m, n)`.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    at_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Transposed matrix of shape `(n, m)`.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The current implementation of the transpose operation is relatively inefficient.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.autoinit</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.driver as drv</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.gpuarray as gpuarray</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import linalg</span>
<span class="sd">    &gt;&gt;&gt; linalg.init()</span>
<span class="sd">    &gt;&gt;&gt; a = np.array([[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12]], np.float32)</span>
<span class="sd">    &gt;&gt;&gt; a_gpu = gpuarray.to_gpu(a)</span>
<span class="sd">    &gt;&gt;&gt; at_gpu = linalg.hermitian(a_gpu)</span>
<span class="sd">    &gt;&gt;&gt; np.all(a.T == at_gpu.get())</span>
<span class="sd">    True</span>
<span class="sd">    &gt;&gt;&gt; b = np.array([[1j, 2j, 3j, 4j, 5j, 6j], [7j, 8j, 9j, 10j, 11j, 12j]], np.complex64)</span>
<span class="sd">    &gt;&gt;&gt; b_gpu = gpuarray.to_gpu(b)</span>
<span class="sd">    &gt;&gt;&gt; bt_gpu = linalg.hermitian(b_gpu)</span>
<span class="sd">    &gt;&gt;&gt; np.all(np.conj(b.T) == bt_gpu.get())</span>
<span class="sd">    True</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;unrecognized type&#39;</span><span class="p">)</span>

    <span class="n">use_double</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">])</span>
    <span class="n">use_complex</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">])</span>

    <span class="c"># Get block/grid sizes:</span>
    <span class="n">dev</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">get_current_device</span><span class="p">()</span>
    <span class="n">block_dim</span><span class="p">,</span> <span class="n">grid_dim</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">select_block_grid_sizes</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c"># Set this to False when debugging to make sure the compiled kernel is</span>
    <span class="c"># not cached:</span>
    <span class="n">cache_dir</span><span class="o">=</span><span class="bp">None</span>
    <span class="n">transpose_mod</span> <span class="o">=</span> \
                  <span class="n">SourceModule</span><span class="p">(</span><span class="n">transpose_template</span><span class="o">.</span><span class="n">substitute</span><span class="p">(</span><span class="n">use_double</span><span class="o">=</span><span class="n">use_double</span><span class="p">,</span>
                                                             <span class="n">use_complex</span><span class="o">=</span><span class="n">use_complex</span><span class="p">,</span>
                                                             <span class="n">hermitian</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                               <span class="n">cols</span><span class="o">=</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                               <span class="n">rows</span><span class="o">=</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                               <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">)</span>

    <span class="n">transpose</span> <span class="o">=</span> <span class="n">transpose_mod</span><span class="o">.</span><span class="n">get_function</span><span class="p">(</span><span class="s">&quot;transpose&quot;</span><span class="p">)</span>
    <span class="n">at_gpu</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">transpose</span><span class="p">(</span><span class="n">at_gpu</span><span class="p">,</span> <span class="n">a_gpu</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">(</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
              <span class="n">block</span><span class="o">=</span><span class="n">block_dim</span><span class="p">,</span>
              <span class="n">grid</span><span class="o">=</span><span class="n">grid_dim</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">at_gpu</span>
</div>
<div class="viewcode-block" id="conj"><a class="viewcode-back" href="../../../generated/scikits.cuda.linalg.conj.html#scikits.cuda.linalg.conj">[docs]</a><span class="k">def</span> <span class="nf">conj</span><span class="p">(</span><span class="n">x_gpu</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Complex conjugate.</span>

<span class="sd">    Compute the complex conjugate of the array in device memory.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Input array of shape `(m, n)`.</span>
<span class="sd">    overwrite : bool</span>
<span class="sd">        If true (default), save the result in the specified array.</span>
<span class="sd">        If false, return the result in a newly allocated array.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    xc_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Conjugate of the input array. If `overwrite` is true, the</span>
<span class="sd">        returned matrix is the same as the input array.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.driver as drv</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.gpuarray as gpuarray</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.autoinit</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import linalg</span>
<span class="sd">    &gt;&gt;&gt; linalg.init()</span>
<span class="sd">    &gt;&gt;&gt; x = np.array([[1+1j, 2-2j, 3+3j, 4-4j], [5+5j, 6-6j, 7+7j, 8-8j]], np.complex64)</span>
<span class="sd">    &gt;&gt;&gt; x_gpu = gpuarray.to_gpu(x)</span>
<span class="sd">    &gt;&gt;&gt; y_gpu = linalg.conj(x_gpu)</span>
<span class="sd">    &gt;&gt;&gt; np.all(x == np.conj(y_gpu.get()))</span>
<span class="sd">    True</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># Don&#39;t attempt to process non-complex matrix types:</span>
    <span class="k">if</span> <span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">x_gpu</span>
    
    <span class="k">try</span><span class="p">:</span>
        <span class="n">func</span> <span class="o">=</span> <span class="n">conj</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span>
    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
        <span class="n">ctype</span> <span class="o">=</span> <span class="n">tools</span><span class="o">.</span><span class="n">dtype_to_ctype</span><span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">func</span> <span class="o">=</span> <span class="n">el</span><span class="o">.</span><span class="n">ElementwiseKernel</span><span class="p">(</span>
                <span class="s">&quot;{ctype} *x, {ctype} *y&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ctype</span><span class="o">=</span><span class="n">ctype</span><span class="p">),</span>
                <span class="s">&quot;y[i] = conj(x[i])&quot;</span><span class="p">)</span>
        <span class="n">conj</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="n">func</span>
    <span class="k">if</span> <span class="n">overwrite</span><span class="p">:</span>
        <span class="n">func</span><span class="p">(</span><span class="n">x_gpu</span><span class="p">,</span> <span class="n">x_gpu</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_gpu</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">y_gpu</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">x_gpu</span><span class="p">)</span>
        <span class="n">func</span><span class="p">(</span><span class="n">x_gpu</span><span class="p">,</span> <span class="n">y_gpu</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_gpu</span></div>
<span class="n">conj</span><span class="o">.</span><span class="n">cache</span> <span class="o">=</span> <span class="p">{}</span>

<span class="n">diag_template</span> <span class="o">=</span> <span class="n">Template</span><span class="p">(</span><span class="s">&quot;&quot;&quot;</span>
<span class="s">#include &lt;pycuda-complex.hpp&gt;</span>

<span class="s">#if ${use_double}</span>
<span class="s">#if ${use_complex}</span>
<span class="s">#define FLOAT pycuda::complex&lt;double&gt;</span>
<span class="s">#else</span>
<span class="s">#define FLOAT double</span>
<span class="s">#endif</span>
<span class="s">#else</span>
<span class="s">#if ${use_complex}</span>
<span class="s">#define FLOAT pycuda::complex&lt;float&gt;</span>
<span class="s">#else</span>
<span class="s">#define FLOAT float</span>
<span class="s">#endif</span>
<span class="s">#endif</span>

<span class="s">// Assumes that d already contains zeros in all positions.</span>
<span class="s">// N must contain the number of elements in v.</span>
<span class="s">__global__ void diag(FLOAT *v, FLOAT *d, int N) {</span>
<span class="s">    unsigned int idx = blockIdx.y*blockDim.x*gridDim.x+</span>
<span class="s">                       blockIdx.x*blockDim.x+threadIdx.x;</span>
<span class="s">    if (idx &lt; N)</span>
<span class="s">        d[idx*(N+1)] = v[idx];</span>
<span class="s">}</span>

<span class="s">&quot;&quot;&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="diag"><a class="viewcode-back" href="../../../generated/scikits.cuda.linalg.diag.html#scikits.cuda.linalg.diag">[docs]</a><span class="k">def</span> <span class="nf">diag</span><span class="p">(</span><span class="n">v_gpu</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Construct a diagonal matrix if input array is one-dimensional,</span>
<span class="sd">    or extracts diagonal entries of a two-dimensional array.</span>

<span class="sd">    --- If input-array is one-dimensional: </span>
<span class="sd">    Constructs a matrix in device memory whose diagonal elements</span>
<span class="sd">    correspond to the elements in the specified array; all</span>
<span class="sd">    non-diagonal elements are set to 0.</span>
<span class="sd">    </span>
<span class="sd">    --- If input-array is two-dimensional: </span>
<span class="sd">    Constructs an array in device memory whose elements</span>
<span class="sd">    correspond to the elements along the main-diagonal of the specified </span>
<span class="sd">    array.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    v_obj : pycuda.gpuarray.GPUArray</span>
<span class="sd">            Input array of shape `(n,m)`.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    d_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">            ---If v_obj has shape `(n,1)`, output is </span>
<span class="sd">               diagonal matrix of dimensions `[n, n]`.</span>
<span class="sd">            ---If v_obj has shape `(n,m)`, output is </span>
<span class="sd">               array of length `min(n,m)`.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.driver as drv</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.gpuarray as gpuarray</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.autoinit</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import linalg</span>
<span class="sd">    &gt;&gt;&gt; linalg.init()</span>
<span class="sd">    &gt;&gt;&gt; v = np.array([1, 2, 3, 4, 5, 6], np.float32)</span>
<span class="sd">    &gt;&gt;&gt; v_gpu = gpuarray.to_gpu(v)</span>
<span class="sd">    &gt;&gt;&gt; d_gpu = linalg.diag(v_gpu)</span>
<span class="sd">    &gt;&gt;&gt; np.all(d_gpu.get() == np.diag(v))</span>
<span class="sd">    True</span>
<span class="sd">    &gt;&gt;&gt; v = np.array([1j, 2j, 3j, 4j, 5j, 6j], np.complex64)</span>
<span class="sd">    &gt;&gt;&gt; v_gpu = gpuarray.to_gpu(v)</span>
<span class="sd">    &gt;&gt;&gt; d_gpu = linalg.diag(v_gpu)</span>
<span class="sd">    &gt;&gt;&gt; np.all(d_gpu.get() == np.diag(v))</span>
<span class="sd">    True</span>
<span class="sd">    &gt;&gt;&gt; v = np.array([[1., 2., 3.],[4., 5., 6.]], np.float64)</span>
<span class="sd">    &gt;&gt;&gt; v_gpu = gpuarray.to_gpu(v)</span>
<span class="sd">    &gt;&gt;&gt; d_gpu = linalg.diag(v_gpu)</span>
<span class="sd">    &gt;&gt;&gt; d_gpu</span>
<span class="sd">    array([ 1.,  5.])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">v_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;unrecognized type&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">v_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">v_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">):</span>
        <span class="c"># Since CUDA assumes that arrays are stored in column-major</span>
        <span class="c"># format, the input matrix is assumed to be transposed:</span>
        <span class="n">n</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">v_gpu</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">square</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">==</span> <span class="n">m</span><span class="p">)</span>

        <span class="c"># Allocate the output array</span>
        <span class="n">d_gpu</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">v_gpu</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>

        <span class="n">diag_kernel</span> <span class="o">=</span> <span class="n">el</span><span class="o">.</span><span class="n">ElementwiseKernel</span><span class="p">(</span><span class="s">&quot;double *x, double *y, int z&quot;</span><span class="p">,</span> <span class="s">&quot;y[i] = x[(z+1)*i]&quot;</span><span class="p">,</span> <span class="s">&quot;diakernel&quot;</span><span class="p">)</span>
        <span class="n">diag_kernel</span><span class="p">(</span><span class="n">v_gpu</span><span class="p">,</span><span class="n">d_gpu</span><span class="p">,</span><span class="nb">max</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">d_gpu</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">v_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;input array cannot have greater than 2-dimensions&#39;</span><span class="p">)</span>

    <span class="n">use_double</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">v_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">])</span>
    <span class="n">use_complex</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">v_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">])</span>

    <span class="c"># Initialize output matrix:</span>
    <span class="n">d_gpu</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">v_gpu</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">v_gpu</span><span class="o">.</span><span class="n">size</span><span class="p">),</span> <span class="n">v_gpu</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c"># Get block/grid sizes:</span>
    <span class="n">dev</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">get_current_device</span><span class="p">()</span>
    <span class="n">block_dim</span><span class="p">,</span> <span class="n">grid_dim</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">select_block_grid_sizes</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">d_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c"># Set this to False when debugging to make sure the compiled kernel is</span>
    <span class="c"># not cached:</span>
    <span class="n">cache_dir</span><span class="o">=</span><span class="bp">None</span>
    <span class="n">diag_mod</span> <span class="o">=</span> \
             <span class="n">SourceModule</span><span class="p">(</span><span class="n">diag_template</span><span class="o">.</span><span class="n">substitute</span><span class="p">(</span><span class="n">use_double</span><span class="o">=</span><span class="n">use_double</span><span class="p">,</span>
                                                   <span class="n">use_complex</span><span class="o">=</span><span class="n">use_complex</span><span class="p">),</span>
                          <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">)</span>

    <span class="n">diag</span> <span class="o">=</span> <span class="n">diag_mod</span><span class="o">.</span><span class="n">get_function</span><span class="p">(</span><span class="s">&quot;diag&quot;</span><span class="p">)</span>
    <span class="n">diag</span><span class="p">(</span><span class="n">v_gpu</span><span class="p">,</span> <span class="n">d_gpu</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">(</span><span class="n">v_gpu</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
         <span class="n">block</span><span class="o">=</span><span class="n">block_dim</span><span class="p">,</span>
         <span class="n">grid</span><span class="o">=</span><span class="n">grid_dim</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">d_gpu</span>
</div>
<span class="n">eye_template</span> <span class="o">=</span> <span class="n">Template</span><span class="p">(</span><span class="s">&quot;&quot;&quot;</span>
<span class="s">#include &lt;pycuda-complex.hpp&gt;</span>

<span class="s">#if ${use_double}</span>
<span class="s">#if ${use_complex}</span>
<span class="s">#define FLOAT pycuda::complex&lt;double&gt;</span>
<span class="s">#else</span>
<span class="s">#define FLOAT double</span>
<span class="s">#endif</span>
<span class="s">#else</span>
<span class="s">#if ${use_complex}</span>
<span class="s">#define FLOAT pycuda::complex&lt;float&gt;</span>
<span class="s">#else</span>
<span class="s">#define FLOAT float</span>
<span class="s">#endif</span>
<span class="s">#endif</span>

<span class="s">// Assumes that d already contains zeros in all positions.</span>
<span class="s">// N must contain the number of rows or columns in the matrix.</span>
<span class="s">__global__ void eye(FLOAT *d, int N) {</span>
<span class="s">    unsigned int idx = blockIdx.y*blockDim.x*gridDim.x+</span>
<span class="s">                       blockIdx.x*blockDim.x+threadIdx.x;</span>
<span class="s">    if (idx &lt; N)</span>
<span class="s">        d[idx*(N+1)] = FLOAT(1.0);</span>
<span class="s">}</span>

<span class="s">&quot;&quot;&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="eye"><a class="viewcode-back" href="../../../generated/scikits.cuda.linalg.eye.html#scikits.cuda.linalg.eye">[docs]</a><span class="k">def</span> <span class="nf">eye</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Construct a 2D matrix with ones on the diagonal and zeros elsewhere.</span>

<span class="sd">    Constructs a matrix in device memory whose diagonal elements</span>
<span class="sd">    are set to 1 and non-diagonal elements are set to 0.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    N : int</span>
<span class="sd">        Number of rows or columns in the output matrix.</span>
<span class="sd">    dtype : type</span>
<span class="sd">        Matrix data type.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    e_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Diagonal matrix of dimensions `[N, N]` with diagonal values</span>
<span class="sd">        set to 1.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.driver as drv</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.gpuarray as gpuarray</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.autoinit</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import linalg</span>
<span class="sd">    &gt;&gt;&gt; linalg.init()</span>
<span class="sd">    &gt;&gt;&gt; N = 5</span>
<span class="sd">    &gt;&gt;&gt; e_gpu = linalg.eye(N)</span>
<span class="sd">    &gt;&gt;&gt; np.all(e_gpu.get() == np.eye(N))</span>
<span class="sd">    True</span>
<span class="sd">    &gt;&gt;&gt; e_gpu = linalg.eye(N, np.complex64)</span>
<span class="sd">    &gt;&gt;&gt; np.all(e_gpu.get() == np.eye(N, dtype=np.complex64))</span>
<span class="sd">    True</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;unrecognized type&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;N must be greater than 0&#39;</span><span class="p">)</span>

    <span class="n">use_double</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">])</span>
    <span class="n">use_complex</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">])</span>

    <span class="c"># Initialize output matrix:</span>
    <span class="n">e_gpu</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">dtype</span><span class="p">)</span>

    <span class="c"># Get block/grid sizes:</span>
    <span class="n">dev</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">get_current_device</span><span class="p">()</span>
    <span class="n">block_dim</span><span class="p">,</span> <span class="n">grid_dim</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">select_block_grid_sizes</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">e_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c"># Set this to False when debugging to make sure the compiled kernel is</span>
    <span class="c"># not cached:</span>
    <span class="n">cache_dir</span><span class="o">=</span><span class="bp">None</span>
    <span class="n">eye_mod</span> <span class="o">=</span> \
             <span class="n">SourceModule</span><span class="p">(</span><span class="n">eye_template</span><span class="o">.</span><span class="n">substitute</span><span class="p">(</span><span class="n">use_double</span><span class="o">=</span><span class="n">use_double</span><span class="p">,</span>
                                                   <span class="n">use_complex</span><span class="o">=</span><span class="n">use_complex</span><span class="p">),</span>
                          <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">)</span>

    <span class="n">eye</span> <span class="o">=</span> <span class="n">eye_mod</span><span class="o">.</span><span class="n">get_function</span><span class="p">(</span><span class="s">&quot;eye&quot;</span><span class="p">)</span>
    <span class="n">eye</span><span class="p">(</span><span class="n">e_gpu</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">(</span><span class="n">N</span><span class="p">),</span>
        <span class="n">block</span><span class="o">=</span><span class="n">block_dim</span><span class="p">,</span>
        <span class="n">grid</span><span class="o">=</span><span class="n">grid_dim</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">e_gpu</span>
</div>
<span class="n">cutoff_invert_s_template</span> <span class="o">=</span> <span class="n">Template</span><span class="p">(</span><span class="s">&quot;&quot;&quot;</span>
<span class="s">#if ${use_double}</span>
<span class="s">#define FLOAT double</span>
<span class="s">#else</span>
<span class="s">#define FLOAT float</span>
<span class="s">#endif</span>

<span class="s">// N must equal the length of s:</span>
<span class="s">__global__ void cutoff_invert_s(FLOAT *s, FLOAT *cutoff, unsigned int N) {</span>
<span class="s">    unsigned int idx = blockIdx.y*blockDim.x*gridDim.x+</span>
<span class="s">                       blockIdx.x*blockDim.x+threadIdx.x;</span>

<span class="s">    if (idx &lt; N)</span>
<span class="s">        if (s[idx] &gt; cutoff[0])</span>
<span class="s">            s[idx] = 1/s[idx];</span>
<span class="s">        else</span>
<span class="s">            s[idx] = 0.0;</span>
<span class="s">}</span>
<span class="s">&quot;&quot;&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="pinv"><a class="viewcode-back" href="../../../generated/scikits.cuda.linalg.pinv.html#scikits.cuda.linalg.pinv">[docs]</a><span class="k">def</span> <span class="nf">pinv</span><span class="p">(</span><span class="n">a_gpu</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="mf">1e-15</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Moore-Penrose pseudoinverse.</span>

<span class="sd">    Compute the Moore-Penrose pseudoinverse of the specified matrix.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Input matrix of shape `(m, n)`.</span>
<span class="sd">    rcond : float</span>
<span class="sd">        Singular values smaller than `rcond`*max(singular_values)`</span>
<span class="sd">        are set to zero.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    a_inv_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Pseudoinverse of input matrix.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Double precision is only supported if the standard version of the</span>
<span class="sd">    CULA Dense toolkit is installed.</span>

<span class="sd">    This function destroys the contents of the input matrix.</span>

<span class="sd">    If the input matrix is square, the pseudoinverse uses less memory.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.driver as drv</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.gpuarray as gpuarray</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.autoinit</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import linalg</span>
<span class="sd">    &gt;&gt;&gt; linalg.init()</span>
<span class="sd">    &gt;&gt;&gt; a = np.asarray(np.random.rand(8, 4), np.float32)</span>
<span class="sd">    &gt;&gt;&gt; a_gpu = gpuarray.to_gpu(a)</span>
<span class="sd">    &gt;&gt;&gt; a_inv_gpu = linalg.pinv(a_gpu)</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(np.linalg.pinv(a), a_inv_gpu.get(), 1e-4)</span>
<span class="sd">    True</span>
<span class="sd">    &gt;&gt;&gt; b = np.asarray(np.random.rand(8, 4)+1j*np.random.rand(8, 4), np.complex64)</span>
<span class="sd">    &gt;&gt;&gt; b_gpu = gpuarray.to_gpu(b)</span>
<span class="sd">    &gt;&gt;&gt; b_inv_gpu = linalg.pinv(b_gpu)</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(np.linalg.pinv(b), b_inv_gpu.get(), 1e-4)</span>
<span class="sd">    True</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">_has_cula</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s">&#39;CULA not installed&#39;</span><span class="p">)</span>

    <span class="c"># Perform in-place SVD if the matrix is square to save memory:</span>
    <span class="k">if</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">u_gpu</span><span class="p">,</span> <span class="n">s_gpu</span><span class="p">,</span> <span class="n">vh_gpu</span> <span class="o">=</span> <span class="n">svd</span><span class="p">(</span><span class="n">a_gpu</span><span class="p">,</span> <span class="s">&#39;s&#39;</span><span class="p">,</span> <span class="s">&#39;o&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">u_gpu</span><span class="p">,</span> <span class="n">s_gpu</span><span class="p">,</span> <span class="n">vh_gpu</span> <span class="o">=</span> <span class="n">svd</span><span class="p">(</span><span class="n">a_gpu</span><span class="p">,</span> <span class="s">&#39;s&#39;</span><span class="p">,</span> <span class="s">&#39;s&#39;</span><span class="p">)</span>

    <span class="c"># Get block/grid sizes; the number of threads per block is limited</span>
    <span class="c"># to 512 because the cutoff_invert_s kernel defined above uses too</span>
    <span class="c"># many registers to be invoked in 1024 threads per block (i.e., on</span>
    <span class="c"># GPUs with compute capability &gt;= 2.x):</span>
    <span class="n">dev</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">get_current_device</span><span class="p">()</span>
    <span class="n">max_threads_per_block</span> <span class="o">=</span> <span class="mi">512</span>
    <span class="n">block_dim</span><span class="p">,</span> <span class="n">grid_dim</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">select_block_grid_sizes</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">s_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">max_threads_per_block</span><span class="p">)</span>

    <span class="c"># Suppress very small singular values:</span>
    <span class="n">use_double</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">s_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="n">cutoff_invert_s_mod</span> <span class="o">=</span> \
        <span class="n">SourceModule</span><span class="p">(</span><span class="n">cutoff_invert_s_template</span><span class="o">.</span><span class="n">substitute</span><span class="p">(</span><span class="n">use_double</span><span class="o">=</span><span class="n">use_double</span><span class="p">))</span>
    <span class="n">cutoff_invert_s</span> <span class="o">=</span> \
                    <span class="n">cutoff_invert_s_mod</span><span class="o">.</span><span class="n">get_function</span><span class="p">(</span><span class="s">&#39;cutoff_invert_s&#39;</span><span class="p">)</span>
    <span class="n">cutoff_gpu</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">s_gpu</span><span class="p">)</span><span class="o">*</span><span class="n">rcond</span>
    <span class="n">cutoff_invert_s</span><span class="p">(</span><span class="n">s_gpu</span><span class="p">,</span> <span class="n">cutoff_gpu</span><span class="p">,</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">(</span><span class="n">s_gpu</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
                    <span class="n">block</span><span class="o">=</span><span class="n">block_dim</span><span class="p">,</span> <span class="n">grid</span><span class="o">=</span><span class="n">grid_dim</span><span class="p">)</span>

    <span class="c"># Compute the pseudoinverse without allocating a new diagonal matrix:</span>
    <span class="k">return</span> <span class="n">dot</span><span class="p">(</span><span class="n">vh_gpu</span><span class="p">,</span> <span class="n">dot_diag</span><span class="p">(</span><span class="n">s_gpu</span><span class="p">,</span> <span class="n">u_gpu</span><span class="p">,</span> <span class="s">&#39;t&#39;</span><span class="p">),</span> <span class="s">&#39;c&#39;</span><span class="p">,</span> <span class="s">&#39;c&#39;</span><span class="p">)</span>
</div>
<span class="n">tril_template</span> <span class="o">=</span> <span class="n">Template</span><span class="p">(</span><span class="s">&quot;&quot;&quot;</span>
<span class="s">#include &lt;pycuda-complex.hpp&gt;</span>

<span class="s">#if ${use_double}</span>
<span class="s">#if ${use_complex}</span>
<span class="s">#define FLOAT pycuda::complex&lt;double&gt;</span>
<span class="s">#else</span>
<span class="s">#define FLOAT double</span>
<span class="s">#endif</span>
<span class="s">#else</span>
<span class="s">#if ${use_complex}</span>
<span class="s">#define FLOAT pycuda::complex&lt;float&gt;</span>
<span class="s">#else</span>
<span class="s">#define FLOAT float</span>
<span class="s">#endif</span>
<span class="s">#endif</span>

<span class="s">__global__ void tril(FLOAT *a, unsigned int N) {</span>
<span class="s">    unsigned int idx = blockIdx.y*blockDim.x*gridDim.x+</span>
<span class="s">                       blockIdx.x*blockDim.x+threadIdx.x;</span>
<span class="s">    unsigned int ix = idx/${cols};</span>
<span class="s">    unsigned int iy = idx%${cols};</span>

<span class="s">    if (idx &lt; N) {</span>
<span class="s">        if (ix &lt; iy)</span>
<span class="s">            a[idx] = 0.0;</span>
<span class="s">    }</span>
<span class="s">}</span>
<span class="s">&quot;&quot;&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="tril"><a class="viewcode-back" href="../../../generated/scikits.cuda.linalg.tril.html#scikits.cuda.linalg.tril">[docs]</a><span class="k">def</span> <span class="nf">tril</span><span class="p">(</span><span class="n">a_gpu</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">handle</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Lower triangle of a matrix.</span>

<span class="sd">    Return the lower triangle of a square matrix.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Input matrix of shape `(m, m)`</span>
<span class="sd">    overwrite : boolean</span>
<span class="sd">        If true (default), zero out the upper triangle of the matrix.</span>
<span class="sd">        If false, return the result in a newly allocated matrix.</span>
<span class="sd">    handle : int</span>
<span class="sd">        CUBLAS context. If no context is specified, the default handle from</span>
<span class="sd">        `scikits.cuda.misc._global_cublas_handle` is used.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    l_gpu : pycuda.gpuarray</span>
<span class="sd">        The lower triangle of the original matrix.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.driver as drv</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.gpuarray as gpuarray</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.autoinit</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import linalg</span>
<span class="sd">    &gt;&gt;&gt; linalg.init()</span>
<span class="sd">    &gt;&gt;&gt; a = np.asarray(np.random.rand(4, 4), np.float32)</span>
<span class="sd">    &gt;&gt;&gt; a_gpu = gpuarray.to_gpu(a)</span>
<span class="sd">    &gt;&gt;&gt; l_gpu = linalg.tril(a_gpu, False)</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(np.tril(a), l_gpu.get())</span>
<span class="sd">    True</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">handle</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">_global_cublas_handle</span>
        
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;matrix must be square&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
        <span class="n">swap_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasSswap</span>
        <span class="n">copy_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasScopy</span>
        <span class="n">use_double</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">use_complex</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
        <span class="n">swap_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasDswap</span>
        <span class="n">copy_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasDcopy</span>
        <span class="n">use_double</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">use_complex</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">:</span>
        <span class="n">swap_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasCswap</span>
        <span class="n">copy_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasCcopy</span>
        <span class="n">use_double</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">use_complex</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">:</span>
        <span class="n">swap_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasZswap</span>
        <span class="n">copy_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasZcopy</span>
        <span class="n">use_double</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">use_complex</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;unrecognized type&#39;</span><span class="p">)</span>

    <span class="n">N</span> <span class="o">=</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c"># Get block/grid sizes:</span>
    <span class="n">dev</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">get_current_device</span><span class="p">()</span>
    <span class="n">block_dim</span><span class="p">,</span> <span class="n">grid_dim</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">select_block_grid_sizes</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c"># Set this to False when debugging to make sure the compiled kernel is</span>
    <span class="c"># not cached:</span>
    <span class="n">cache_dir</span><span class="o">=</span><span class="bp">None</span>
    <span class="n">tril_mod</span> <span class="o">=</span> \
             <span class="n">SourceModule</span><span class="p">(</span><span class="n">tril_template</span><span class="o">.</span><span class="n">substitute</span><span class="p">(</span><span class="n">use_double</span><span class="o">=</span><span class="n">use_double</span><span class="p">,</span>
                                                   <span class="n">use_complex</span><span class="o">=</span><span class="n">use_complex</span><span class="p">,</span>
                                                   <span class="n">cols</span><span class="o">=</span><span class="n">N</span><span class="p">),</span>
                          <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">)</span>
    <span class="n">tril</span> <span class="o">=</span> <span class="n">tril_mod</span><span class="o">.</span><span class="n">get_function</span><span class="p">(</span><span class="s">&quot;tril&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">overwrite</span><span class="p">:</span>
        <span class="n">a_orig_gpu</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">copy_func</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">a_orig_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">tril</span><span class="p">(</span><span class="n">a_gpu</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">(</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
         <span class="n">block</span><span class="o">=</span><span class="n">block_dim</span><span class="p">,</span>
         <span class="n">grid</span><span class="o">=</span><span class="n">grid_dim</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">overwrite</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a_gpu</span>
    <span class="k">else</span><span class="p">:</span>

        <span class="c"># Restore original contents of a_gpu:</span>
        <span class="n">swap_func</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">a_gpu</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">a_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">a_orig_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">a_orig_gpu</span>
</div>
<span class="n">multiply_template</span> <span class="o">=</span> <span class="n">Template</span><span class="p">(</span><span class="s">&quot;&quot;&quot;</span>
<span class="s">#include &lt;pycuda-complex.hpp&gt;</span>

<span class="s">#if ${use_double}</span>
<span class="s">#if ${use_complex}</span>
<span class="s">#define FLOAT pycuda::complex&lt;double&gt;</span>
<span class="s">#else</span>
<span class="s">#define FLOAT double</span>
<span class="s">#endif</span>
<span class="s">#else</span>
<span class="s">#if ${use_complex}</span>
<span class="s">#define FLOAT pycuda::complex&lt;float&gt;</span>
<span class="s">#else</span>
<span class="s">#define FLOAT float</span>
<span class="s">#endif</span>
<span class="s">#endif</span>

<span class="s">// Stores result in y</span>
<span class="s">__global__ void multiply_inplace(FLOAT *x, FLOAT *y,</span>
<span class="s">                                 unsigned int N) {</span>
<span class="s">    unsigned int idx = blockIdx.y*blockDim.x*gridDim.x+</span>
<span class="s">                       blockIdx.x*blockDim.x+threadIdx.x;</span>
<span class="s">    if (idx &lt; N) {</span>
<span class="s">        y[idx] *= x[idx];</span>
<span class="s">    }</span>
<span class="s">}</span>

<span class="s">// Stores result in z</span>
<span class="s">__global__ void multiply(FLOAT *x, FLOAT *y, FLOAT *z,</span>
<span class="s">                         unsigned int N) {</span>
<span class="s">    unsigned int idx = blockIdx.y*blockDim.x*gridDim.x+</span>
<span class="s">                       blockIdx.x*blockDim.x+threadIdx.x;</span>
<span class="s">    if (idx &lt; N) {</span>
<span class="s">        z[idx] = x[idx]*y[idx];</span>
<span class="s">    }</span>
<span class="s">}</span>
<span class="s">&quot;&quot;&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="multiply"><a class="viewcode-back" href="../../../generated/scikits.cuda.linalg.multiply.html#scikits.cuda.linalg.multiply">[docs]</a><span class="k">def</span> <span class="nf">multiply</span><span class="p">(</span><span class="n">x_gpu</span><span class="p">,</span> <span class="n">y_gpu</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiply arguments element-wise.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x_gpu, y_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Input arrays to be multiplied.</span>
<span class="sd">    dev : pycuda.driver.Device</span>
<span class="sd">        Device object to be used.</span>
<span class="sd">    overwrite : bool</span>
<span class="sd">        If true (default), return the result in `y_gpu`.</span>
<span class="sd">        is false, return the result in a newly allocated array.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    z_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        The element-wise product of the input arrays.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.autoinit</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.gpuarray as gpuarray</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import linalg</span>
<span class="sd">    &gt;&gt;&gt; linalg.init()</span>
<span class="sd">    &gt;&gt;&gt; x = np.asarray(np.random.rand(4, 4), np.float32)</span>
<span class="sd">    &gt;&gt;&gt; y = np.asarray(np.random.rand(4, 4), np.float32)</span>
<span class="sd">    &gt;&gt;&gt; x_gpu = gpuarray.to_gpu(x)</span>
<span class="sd">    &gt;&gt;&gt; y_gpu = gpuarray.to_gpu(y)</span>
<span class="sd">    &gt;&gt;&gt; z_gpu = linalg.multiply(x_gpu, y_gpu)</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(x*y, z_gpu.get())</span>
<span class="sd">    True</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">x_gpu</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">y_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;input arrays must have the same shape&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;unrecognized type&#39;</span><span class="p">)</span>

    <span class="n">use_double</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">])</span>
    <span class="n">use_complex</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">])</span>

    <span class="c"># Get block/grid sizes:</span>
    <span class="n">dev</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">get_current_device</span><span class="p">()</span>
    <span class="n">block_dim</span><span class="p">,</span> <span class="n">grid_dim</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">select_block_grid_sizes</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">x_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c"># Set this to False when debugging to make sure the compiled kernel is</span>
    <span class="c"># not cached:</span>
    <span class="n">cache_dir</span><span class="o">=</span><span class="bp">None</span>
    <span class="n">multiply_mod</span> <span class="o">=</span> \
             <span class="n">SourceModule</span><span class="p">(</span><span class="n">multiply_template</span><span class="o">.</span><span class="n">substitute</span><span class="p">(</span><span class="n">use_double</span><span class="o">=</span><span class="n">use_double</span><span class="p">,</span>
                                                       <span class="n">use_complex</span><span class="o">=</span><span class="n">use_complex</span><span class="p">),</span>
                          <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">overwrite</span><span class="p">:</span>
        <span class="n">multiply</span> <span class="o">=</span> <span class="n">multiply_mod</span><span class="o">.</span><span class="n">get_function</span><span class="p">(</span><span class="s">&quot;multiply_inplace&quot;</span><span class="p">)</span>
        <span class="n">multiply</span><span class="p">(</span><span class="n">x_gpu</span><span class="p">,</span> <span class="n">y_gpu</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
                 <span class="n">block</span><span class="o">=</span><span class="n">block_dim</span><span class="p">,</span>
                 <span class="n">grid</span><span class="o">=</span><span class="n">grid_dim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_gpu</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">multiply</span> <span class="o">=</span> <span class="n">multiply_mod</span><span class="o">.</span><span class="n">get_function</span><span class="p">(</span><span class="s">&quot;multiply&quot;</span><span class="p">)</span>
        <span class="n">z_gpu</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">multiply</span><span class="p">(</span><span class="n">x_gpu</span><span class="p">,</span> <span class="n">y_gpu</span><span class="p">,</span> <span class="n">z_gpu</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
                 <span class="n">block</span><span class="o">=</span><span class="n">block_dim</span><span class="p">,</span>
                 <span class="n">grid</span><span class="o">=</span><span class="n">grid_dim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z_gpu</span>
</div>
<div class="viewcode-block" id="norm"><a class="viewcode-back" href="../../../generated/scikits.cuda.linalg.norm.html#scikits.cuda.linalg.norm">[docs]</a><span class="k">def</span> <span class="nf">norm</span><span class="p">(</span><span class="n">x_gpu</span><span class="p">,</span> <span class="n">handle</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Euclidean norm (2-norm) of real vector.</span>

<span class="sd">    Computes the Euclidean norm of an array.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Input array.</span>
<span class="sd">    handle : int</span>
<span class="sd">        CUBLAS context. If no context is specified, the default handle from</span>
<span class="sd">        `scikits.misc._global_cublas_handle` is used.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    nrm : real</span>
<span class="sd">        Euclidean norm of `x`.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.autoinit</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.gpuarray as gpuarray</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import linalg</span>
<span class="sd">    &gt;&gt;&gt; linalg.init()</span>
<span class="sd">    &gt;&gt;&gt; x = np.asarray(np.random.rand(4, 4), np.float32)</span>
<span class="sd">    &gt;&gt;&gt; x_gpu = gpuarray.to_gpu(x)</span>
<span class="sd">    &gt;&gt;&gt; nrm = linalg.norm(x_gpu)</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(nrm, np.linalg.norm(x))</span>
<span class="sd">    True</span>
<span class="sd">    &gt;&gt;&gt; x_gpu = gpuarray.to_gpu(np.array([3+4j, 12-84j]))</span>
<span class="sd">    &gt;&gt;&gt; linalg.norm(x_gpu)</span>
<span class="sd">    85.0</span>

<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">if</span> <span class="n">handle</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">_global_cublas_handle</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">x_gpu</span> <span class="o">=</span> <span class="n">x_gpu</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="c"># Compute inner product for 1D arrays:</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">):</span>
        <span class="n">cublas_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasScnrm2</span>
    <span class="k">elif</span> <span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="n">cublas_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasSnrm2</span>
    <span class="k">elif</span> <span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">):</span>
        <span class="n">cublas_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasDznrm2</span>
    <span class="k">elif</span> <span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
        <span class="n">cublas_func</span> <span class="o">=</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasDnrm2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;unsupported input type&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">cublas_func</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">x_gpu</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">x_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
</div>
<span class="k">def</span> <span class="nf">scale</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">x_gpu</span><span class="p">,</span> <span class="n">alpha_real</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">handle</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Scale a vector by a factor alpha.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    alpha : scalar</span>
<span class="sd">        Scale parameter</span>
<span class="sd">    x_gpu : pycuda.gpuarray.GPUArray</span>
<span class="sd">        Input array.</span>
<span class="sd">    alpha_real : bool</span>
<span class="sd">        If `True` and `x_gpu` is complex, then one of the specialized versions </span>
<span class="sd">        `cublasCsscal` or `cublasZdscal` is used which might improve</span>
<span class="sd">        performance for large arrays.  (By default, `alpha` is coerced to</span>
<span class="sd">        the corresponding complex type.) </span>
<span class="sd">    handle : int</span>
<span class="sd">        CUBLAS context. If no context is specified, the default handle from</span>
<span class="sd">        `scikits.misc._global_cublas_handle` is used.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.autoinit</span>
<span class="sd">    &gt;&gt;&gt; import pycuda.gpuarray as gpuarray</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import linalg</span>
<span class="sd">    &gt;&gt;&gt; linalg.init()</span>
<span class="sd">    &gt;&gt;&gt; x = np.asarray(np.random.rand(4, 4), np.float32)</span>
<span class="sd">    &gt;&gt;&gt; x_gpu = gpuarray.to_gpu(x)</span>
<span class="sd">    &gt;&gt;&gt; alpha = 2.4</span>
<span class="sd">    &gt;&gt;&gt; linalg.scale(alpha, x_gpu)</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(x_gpu.get(), alpha*x)</span>
<span class="sd">    True</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">if</span> <span class="n">handle</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">_global_cublas_handle</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">x_gpu</span> <span class="o">=</span> <span class="n">x_gpu</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="n">cublas_func</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasSscal</span><span class="p">,</span>
        <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasDscal</span><span class="p">,</span>
        <span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">:</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasCsscal</span> <span class="k">if</span> <span class="n">alpha_real</span> <span class="k">else</span> 
                      <span class="n">cublas</span><span class="o">.</span><span class="n">cublasCscal</span><span class="p">,</span> 
        <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">:</span> <span class="n">cublas</span><span class="o">.</span><span class="n">cublasZdscal</span> <span class="k">if</span> <span class="n">alpha_real</span> <span class="k">else</span> 
                       <span class="n">cublas</span><span class="o">.</span><span class="n">cublasZscal</span> 
    <span class="p">}</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x_gpu</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">type</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">cublas_func</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">cublas_func</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">x_gpu</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">x_gpu</span><span class="o">.</span><span class="n">gpudata</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;unsupported input type&#39;</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">doctest</span>
    <span class="n">doctest</span><span class="o">.</span><span class="n">testmod</span><span class="p">()</span>
</pre></div>

    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2009-2014, Lev Givon.<br/>
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.2.2.<br/>
    </p>
  </div>
</footer>
  </body>
</html>